{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. **Maximum wind speed prediction at the Sprogø station**\n",
    "\n",
    "The exercise goal is to predict the maximum wind speed occurring every 50 years even if no measure exists for such a period. The available data are only measured over 21 years at the Sprogø meteorological station located in Denmark. \n",
    "\n",
    "The annual maxima are supposed to fit a normal probability density function. However such function is not going to be estimated because it gives a probability from a wind speed maxima. Finding the maximum wind speed occurring every 50 years requires the opposite approach, the result needs to be found from a defined probability. That is the quantile function role and the exercise goal will be to find it. In the current model, it is supposed that the maximum wind speed occurring every 50 years is defined as the upper 2% quantile.\n",
    "\n",
    "By definition, the quantile function is the inverse of the cumulative distribution function. The latter describes the probability distribution of an annual maxima. In the exercise, the cumulative probability $p_i$ for a given year i is defined as $p_i = i/(N+1)$ with $N = 21$, the number of measured years. Thus it will be possible to calculate the cumulative probability of every measured wind speed maxima. From those experimental points, the scipy.interpolate module will be very useful for fitting the quantile function. Finally the 50 years maxima is going to be evaluated from the cumulative probability of the 2% quantile.\n",
    "\n",
    "Practically, load the dataset:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "max_speeds = np.load('max-speeds.npy')\n",
    "years_nb = max_speeds.shape[0]\n",
    "```\n",
    "\n",
    "Compute then the cumulative probability $p_i$ (`cprob`) and sort the maximum speeds from the data. Use then the  UnivariateSpline from scipy.interpolate to define a quantile function and thus estimate the probabilities.\n",
    "\n",
    "In the current model, the maximum wind speed occurring every 50 years is defined as the upper 2% quantile. As a result, the cumulative probability value will be:\n",
    "\n",
    "```python\n",
    "fifty_prob = 1. - 0.02\n",
    "```\n",
    "\n",
    "So the storm wind speed occurring every 50 years can be guessed as:\n",
    "\n",
    "``` python\n",
    "fifty_wind = quantile_func(fifty_prob)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relatively guided, straightforward.\n",
    "# \"Q, quantile\" is the number (function of p) st the prob of the random variable being less or eq to Q is equal to p\n",
    "# so instead of fitting a gaussian (binned wind speed on x axis and occurences, probabilities if normalised,  on y axis),\n",
    "# we fit Q(p) = sqrt(2) erf^-1(2p-1) as per Wikipedia (or rather a generic function?); \n",
    "# where p(x_i), after arranging dataset in ascending order, is the fraction of measures less or equal to x_i "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from scipy import (the used submodules) as ...\n",
    "max_speeds = np.load('max-speeds.npy')\n",
    "years_nb = max_speeds.shape[0]\n",
    "print(max_speeds, '\\n', years_nb)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. **Curve fitting of temperature in Alaska** \n",
    "\n",
    "The temperature extremes in Alaska for each month, starting in January, are given by (in degrees Celcius):\n",
    "\n",
    "max:  17,  19,  21,  28,  33,  38, 37,  37,  31,  23,  19,  18\n",
    "\n",
    "min: -62, -59, -56, -46, -32, -18, -9, -13, -25, -46, -52, -58\n",
    "\n",
    "* Plot these temperature extremes.\n",
    "* Define a function that can describe min and max temperatures. \n",
    "* Fit this function to the data with scipy.optimize.curve_fit().\n",
    "* Plot the result. Is the fit reasonable? If not, why?\n",
    "* Is the time offset for min and max temperatures the same within the fit accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1- use pyplot;\n",
    "#2/5- as daylight is a sine function of time of the year (and latitude and possibly longitude),\n",
    "# makes sense to use it to fit eg the max temperatures(?)\n",
    "# also should use two of them, as it's clear that thermal excursion is not contant;\n",
    "# so the function for min temperature should have the same periodicity bud include a phase (-> diff maxs and mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. **2D minimization of a six-hump camelback function**\n",
    "\n",
    "$$\n",
    "f(x,y) = \\left(4-2.1x^2+\\frac{x^4}{3} \\right) x^2 +xy + (4y^2 -4)y^2\n",
    "$$\n",
    "\n",
    "has multiple global and local minima. Find the global minima of this function.\n",
    "\n",
    "Hints:\n",
    "\n",
    "* Variables can be restricted to $-2 < x < 2$ and $-1 < y < 1$.\n",
    "* Use numpy.meshgrid() and pylab.imshow() to find visually the regions.\n",
    "* Use scipy.optimize.minimize(), optionally trying out several of its methods.\n",
    "\n",
    "How many global minima are there, and what is the function value at those points? What happens for an initial guess of $(x, y) = (0, 0)$ ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# straightforward seeing last class: either \"by hand\" draw multiple intervals and use Brent optimization over each and then argmin, or\n",
    "# use \"basinhopping\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. **FFT of a simple dataset**\n",
    "\n",
    "Performe a periodicity analysis on the lynxs-hares population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use fFT, very much the same procedures as last part of notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. **FFT of an image**\n",
    "\n",
    "* Examine the provided image `moonlanding.png`, which is heavily contaminated with periodic noise. In this exercise, we aim to clean up the noise using the Fast Fourier Transform.\n",
    "* Load the image using pylab.imread().\n",
    "* Find and use the 2-D FFT function in scipy.fftpack, and plot the spectrum (Fourier transform of) the image. Do you have any trouble visualising the spectrum? If so, why?\n",
    "* The spectrum consists of high and low frequency components. The noise is contained in the high-frequency part of the spectrum, so set some of those components to zero (use array slicing).\n",
    "* Apply the inverse Fourier transform to see the resulting image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1/3- 2d Fourier transform are pretty obviously defined (see Wiki),\n",
    "# however it will be a function of two variables (two \"frequencies\"), so should prob either\n",
    "# keep one fixed and plot two graphs, or use a 3d plot.\n",
    "#4- easiest to do it \"by hand\", algo to understand which is noise would be much more complicated(?)\n",
    "#5- ie plot the function as a truncated Fourier expansion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
